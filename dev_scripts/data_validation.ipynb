{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import json\n",
    "\n",
    "MODELS = [\n",
    "    \"models/all-MiniLM-L6-v2\",\n",
    "    \"/home/data/ldrs_analytics/models/all-MiniLM-L6-v2-train_nli-boc_17398_epoch_100_lr_1e-05\",\n",
    "    \"/home/data/ldrs_analytics/models/all-MiniLM-L6-v2-train_sts-boc_17398_epoch_100_lr_1e-05\"\n",
    "]\n",
    "MODEL_PATH = MODELS[0]\n",
    "SIM_MODEL = SentenceTransformer(MODEL_PATH)\n",
    "\n",
    "def timeit(func):\n",
    "    from functools import wraps\n",
    "    import time\n",
    "    LOG_DIR = '/home/data/ldrs_analytics/data/log'\n",
    "\n",
    "    @wraps(func)\n",
    "    def timeit_wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        total_time = end_time - start_time\n",
    "        row = {'task': func.__name__,\n",
    "               'filename': args[0].fname,\n",
    "               'runtime': total_time}\n",
    "        log2csv(LOG_DIR + '/log_term_matching.csv', row)\n",
    "        # print(f'Function {func.__name__}{args} {kwargs} Took {total_time:.4f} seconds')\n",
    "        return result\n",
    "    return timeit_wrapper\n",
    "\n",
    "@timeit\n",
    "def log2csv(csv_name, row):\n",
    "    '''\n",
    "    log the record 'row' into path 'csv_name'\n",
    "    '''\n",
    "    import csv\n",
    "    import os.path\n",
    "\n",
    "    file_exists = os.path.isfile(csv_name)\n",
    "    # Open the CSV file in \"append\" mode\n",
    "    with open(csv_name, 'a', newline='') as f:\n",
    "        # Create a dictionary writer with the dict keys as column fieldnames\n",
    "        writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader() # file doesn't exist yet, write a header\n",
    "            # Append single row to CSV\n",
    "        writer.writerow(row)\n",
    "\n",
    "def process_str(s):\n",
    "    if not s or s == np.nan:\n",
    "        return None\n",
    "    s = re.sub('<s>', '', s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def process_ts_section(section):\n",
    "    ''' clean parsed term in TS\n",
    "    '''\n",
    "    processed_section = ''\n",
    "    try:\n",
    "        processed_section = process_str(re.split('  ', section)[0])\n",
    "    except Exception as e:\n",
    "        print(section, e)\n",
    "    return processed_section\n",
    "\n",
    "\n",
    "def get_similarity(\n",
    "    ts_string_list,\n",
    "    fa_string_list,\n",
    "    map_type,\n",
    "    sim_threshold=0.6,\n",
    "    model_path=MODEL_PATH,\n",
    "    pretrained=SIM_MODEL,\n",
    "    top_N=5\n",
    "):\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    import torch\n",
    "    \n",
    "    ts_string_list = [str(i) for i in ts_string_list]\n",
    "    fa_string_list = [str(i) for i in fa_string_list]\n",
    "    \n",
    "    if pretrained is not None:\n",
    "        # model = SentenceTransformer(pretrained, device=torch.device(\"cuda\", 2))\n",
    "        model = pretrained\n",
    "    else:\n",
    "        model = SentenceTransformer(model_path, device=torch.device(\"cuda\", 2))\n",
    "        \n",
    "    embeddings1 = model.encode(ts_string_list, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(fa_string_list, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    \n",
    "    similar_pairs = {}\n",
    "    for i in range(len(ts_string_list)):\n",
    "        all_score = list(cosine_scores[i])\n",
    "        above_threshold_idx = [all_score.index(k) for k in [j for j in all_score if j >= sim_threshold]]\n",
    "        above_threshold_sims = [j.item() for j in all_score if j >= sim_threshold]\n",
    "        idx_sims = list(zip(above_threshold_idx, above_threshold_sims))\n",
    "\n",
    "        idx_sims = sorted(idx_sims, key=lambda x: x[1], reverse=True)[:top_N]\n",
    "        ref_string = []\n",
    "#         sim_score = []\n",
    "        \n",
    "        map_results = []\n",
    "        for idx, sims in idx_sims:\n",
    "            string = fa_string_list[idx]\n",
    "            if string not in ref_string:\n",
    "                ref_string.append(string)\n",
    "                map_results.append({\n",
    "                    'similar_term': string,\n",
    "                    'score': round(sims, 2),\n",
    "                    'map_type': map_type\n",
    "                })\n",
    "        if ref_string:\n",
    "            similar_pairs.update({\n",
    "                ts_string_list[i]: map_results\n",
    "            })\n",
    "    return similar_pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"17_NBFI_SYN_TS_mkd_20221123_docparse.csv\"\n",
    "TS_folderpath = FA_folderpath = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  17_NBFI_SYN_TS_mkd_20221123_docparse.csv\n"
     ]
    }
   ],
   "source": [
    "print('Processing: ', f)\n",
    "start_time = time.perf_counter()\n",
    "ts_file = os.path.join(TS_folderpath, f)\n",
    "fa_f = re.sub('_TS_', '_FA_', f)\n",
    "fa_file = os.path.join(FA_folderpath, fa_f)\n",
    "\n",
    "# FA file\n",
    "df_fa = pd.read_csv(fa_file).astype(str)\n",
    "df_fa['index'] = df_fa['index'].astype(int)\n",
    "# parties\n",
    "isPartiesStart = df_fa.text.str.contains('^THIS AGREEMENT is|is made on|^PARTIES|Between:*', na=False, case=False)\n",
    "isPartiesEnd = df_fa.text.str.contains('IT IS AGREED*:*|AGREED* as follows|AGREED* that', na=False, case=False)\n",
    "partiesBeginID = df_fa[isPartiesStart]['index'].values[0] + 1\n",
    "partiesEndID = df_fa[isPartiesEnd]['index'].values[0] - 1\n",
    "parties_clause_id = df_fa['index'].between(isPartiesStart, isPartiesEnd)\n",
    "df_fa.loc[parties_clause_id,'section'] = 'PARTIES'\n",
    "df_fa.loc[parties_clause_id, 'section_id'] = '0'\n",
    "\n",
    "df_fa = df_fa.replace({np.nan: None, 'nan': None, 'None': None})\n",
    "\n",
    "# df_parties = df_fa[(df_fa.section_id == \"0\") | (df_fa.section_id == 0)] # cols: definition + text\n",
    "df_parties = df_fa[df_fa.section=='PARTIES'] # line 149 to string, some section ids are like 0.0, not int.\n",
    "# definition\n",
    "df_def = df_fa[df_fa.sub_section.str.contains('Definition', na=False, case=False)] # cols: definition + text\n",
    "df_def = df_def[~df_def.definition.isnull()]\n",
    "# exclude parties & definition clause\n",
    "df_others = df_fa[\n",
    "    ~(df_fa.section.str.contains(\"INTERPRETATION\", na=False, case=False)) & (df_fa.section_id  != \"0\")\n",
    "]\n",
    "# schedule\n",
    "df_schedule = df_others.loc[df_others.schedule.notnull()]\n",
    "# main clause\n",
    "df_clause = df_others.loc[~df_others.schedule.notnull()]\n",
    "\n",
    "# TS file\n",
    "df_ts = pd.read_csv(ts_file)\n",
    "df_ts[\"processed_section\"] = df_ts[\"section\"] #.apply(lambda i: process_ts_section(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Approvals',\n",
       " 'Availability',\n",
       " 'Borrower',\n",
       " 'Cancellation',\n",
       " 'Commitment Fee Waived. Interest Rate',\n",
       " 'Coordinator',\n",
       " 'Covenants',\n",
       " 'Documentation',\n",
       " 'Facility',\n",
       " 'Facility Amount',\n",
       " 'Governing Law',\n",
       " 'Interest Calculation and Payment',\n",
       " 'Interest Period',\n",
       " 'Lenders',\n",
       " 'Lenders. Agent',\n",
       " 'Mandated Lead Arranger and Bookrunner ',\n",
       " 'Maturity Date',\n",
       " 'Purpose',\n",
       " 'Repayment',\n",
       " 'Signing Date',\n",
       " 'Withholding Taxes',\n",
       " nan,\n",
       " 'the Borrower shall immediately prepay all outstanding loans or that Lenders'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_ts.section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text_block_id</th>\n",
       "      <th>page_id</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>section</th>\n",
       "      <th>text_element</th>\n",
       "      <th>list_id</th>\n",
       "      <th>text</th>\n",
       "      <th>keyphrase</th>\n",
       "      <th>text_granularity</th>\n",
       "      <th>clause_id</th>\n",
       "      <th>definition</th>\n",
       "      <th>schedule_id</th>\n",
       "      <th>annotation</th>\n",
       "      <th>docparse_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>title</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summary of Indicative Principal Terms and Cond...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentence</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/07/2023, 14:15:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This draft term sheet is indicative only and</td>\n",
       "      <td>['draft term sheet']</td>\n",
       "      <td>phrase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/07/2023, 14:15:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>does not constitute a legally binding commitme...</td>\n",
       "      <td>[]</td>\n",
       "      <td>phrase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/07/2023, 14:15:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The purpose of these indicative terms and</td>\n",
       "      <td>['indicative terms']</td>\n",
       "      <td>phrase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/07/2023, 14:15:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conditions is to facilitate further discussion...</td>\n",
       "      <td>[]</td>\n",
       "      <td>phrase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/07/2023, 14:15:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>225</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Market disruption</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>term</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/07/2023, 14:15:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>226</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Break Costs</td>\n",
       "      <td>section</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Break Costs:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentence</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/07/2023, 14:15:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>227</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Break Costs</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If any Loan under the Facility is prepaid in w...</td>\n",
       "      <td>['Facility Agreement']</td>\n",
       "      <td>phrase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/07/2023, 14:15:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>227</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>Break Costs</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>each anniversary of that date, in respect of a...</td>\n",
       "      <td>['demand pay']</td>\n",
       "      <td>phrase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/07/2023, 14:15:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>227</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>Break Costs</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>expenses properly incurred by that Finance Par...</td>\n",
       "      <td>[]</td>\n",
       "      <td>phrase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/07/2023, 14:15:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  text_block_id page_id  phrase_id            section text_element  \\\n",
       "0        0              0       1          0                NaN        title   \n",
       "1        1              0       1          0                NaN    paragraph   \n",
       "2        1              0       1          1                NaN    paragraph   \n",
       "3        1              0       1          2                NaN    paragraph   \n",
       "4        1              0       1          3                NaN    paragraph   \n",
       "..     ...            ...     ...        ...                ...          ...   \n",
       "298    225             55      14          0  Market disruption    paragraph   \n",
       "299    226             56      14          0        Break Costs      section   \n",
       "300    227             56      14          0        Break Costs    paragraph   \n",
       "301    227             56      14          1        Break Costs    paragraph   \n",
       "302    227             56      14          2        Break Costs    paragraph   \n",
       "\n",
       "    list_id                                               text  \\\n",
       "0       NaN  Summary of Indicative Principal Terms and Cond...   \n",
       "1       NaN       This draft term sheet is indicative only and   \n",
       "2       NaN  does not constitute a legally binding commitme...   \n",
       "3       NaN          The purpose of these indicative terms and   \n",
       "4       NaN  conditions is to facilitate further discussion...   \n",
       "..      ...                                                ...   \n",
       "298     NaN                                                NaN   \n",
       "299     NaN                                       Break Costs:   \n",
       "300     NaN  If any Loan under the Facility is prepaid in w...   \n",
       "301     NaN  each anniversary of that date, in respect of a...   \n",
       "302     NaN  expenses properly incurred by that Finance Par...   \n",
       "\n",
       "                  keyphrase text_granularity clause_id definition schedule_id  \\\n",
       "0                       NaN         sentence       NaN        NaN         NaN   \n",
       "1      ['draft term sheet']           phrase       NaN        NaN         NaN   \n",
       "2                        []           phrase       NaN        NaN         NaN   \n",
       "3      ['indicative terms']           phrase       NaN        NaN         NaN   \n",
       "4                        []           phrase       NaN        NaN         NaN   \n",
       "..                      ...              ...       ...        ...         ...   \n",
       "298                      []             term       NaN        NaN         NaN   \n",
       "299                     NaN         sentence       NaN        NaN         NaN   \n",
       "300  ['Facility Agreement']           phrase       NaN        NaN         NaN   \n",
       "301          ['demand pay']           phrase       NaN        NaN         NaN   \n",
       "302                      []           phrase       NaN        NaN         NaN   \n",
       "\n",
       "    annotation     docparse_datetime  \n",
       "0          NaN  10/07/2023, 14:15:52  \n",
       "1          NaN  10/07/2023, 14:15:52  \n",
       "2          NaN  10/07/2023, 14:15:52  \n",
       "3          NaN  10/07/2023, 14:15:52  \n",
       "4          NaN  10/07/2023, 14:15:52  \n",
       "..         ...                   ...  \n",
       "298        NaN  10/07/2023, 14:15:52  \n",
       "299        NaN  10/07/2023, 14:15:52  \n",
       "300        NaN  10/07/2023, 14:15:52  \n",
       "301        NaN  10/07/2023, 14:15:52  \n",
       "302        NaN  10/07/2023, 14:15:52  \n",
       "\n",
       "[303 rows x 15 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### todo: check # of sections in prev version & updated version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='pdf/annotated_ts/78_GF_PRJ_TS_mkd_antd_20210722.pdf'> 27 27 29\n",
      "['Summary of Indicative Terms and Conditions', '1. Borrower', '2. Lender', '3. Facilities  Tranche A', '4. Purpose', '5. Final Maturity Date', '6. Availability Period  Tranche A', '7. Drawdown', '8. Interest Rate', '9. Front-end Fee', '10 . All-in pricing', '11 . Repayment  Tranche A', '12 . Voluntary Prepayment', '13 . Cancellation', '14 . Commitment Fee', '15 . Default Interest', '16 Financial', '. Covenant', '17 . Other Undertakings', '18 . Status', '19 . Expenses', '20 . Increased Cost', '21 . Taxation', '21 . Documentation', '22 . Legal Counsel', '22 . Governing Law', '23 .']\n",
      "['6. Availability', '(A) FATCA Deduction', '3. Facilities', '.  Covenant    that', '(B) FATCA information', '5. Final Maturity', '10 All-in pricing', '21 Taxation', '20 Increased Cost', '22 Legal Counsel', '21 Documentation', '\"FATCA\" means', '19 Expenses', '18 Status', '17 Other', '5) Insert as a new clause', '14 Commitment', '11 Repayment', '13 Cancellation', '22 Governing Law', '12 Voluntary', '15 Default']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Errno 2] No such file or directory: 'data/annotated_ts/bfilled/.DS_Store'\n",
      "<_io.BufferedReader name='pdf/annotated_ts/69_PF_SYN_TS_antd_mkd_20211216.pdf'> 22 22 24\n",
      "['Borrower', 'Coordinator', 'Lenders', 'Facility Agent', 'Facility Amount', 'Purpose', 'Final Maturity Date', 'Repayment', 'Availability Period', 'Commitment Fee', 'Upfront Fee', 'Margin', 'Financial Covenants', 'Mandatory Prepayment  Usual and customary for transactions of this nature, including', 'General Undertakings', 'Conditions Precedent', 'Events of Default', 'Taxes & Other Deductions', 'Costs and Out of Pocket Expenses', 'Material Adverse Change', 'Documentation', 'Governing Law']\n",
      "['7.3                    including', 'Costs and Out of', 'Facility', 'Mandatory Prepayment', 'Purpose 3', 'Taxes & Other', 'Material Adverse']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<_io.BufferedReader name='pdf/annotated_ts/42_AF_SYN_TS_antd_mkd_20180511.pdf'> 24 24 28\n",
      "['Aircraft', 'Delivery Date', 'Borrower and Lessor', 'Lender', 'Facility Agent and Security Trustee', 'Arranger', 'Loan Amount', 'Currency', 'Term of the Loan', 'Availability Period', 'Loan Structure', 'Interest Margin', 'Interest', 'Arrangement Fee', 'Facility Agent and Security Trustee Fee', 'Commitment Fee', 'Default Interest', 'Repayment', 'Security', 'Material Adverse Effect', 'Subleasing', 'Documentation', 'Governing Law', 'Legal Costs']\n",
      "[' Currency', ' Delivery Date', '[Borrower Subsidiary] will be funded', '[BANK] Sole Lender', 'The Loan will be secured by', '[Borrower Subsidiary])', ' Loan Amount', ' Borrower and Lessor', ' Term of the Loan']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Errno 2] No such file or directory: 'data/annotated_ts/bfilled/53_VF_PRJ_TS_mkd_20191018_docparse.csv'\n",
      "<_io.BufferedReader name='pdf/annotated_ts/38_BL_PRJ_TS_mkd_antd_20220526.pdf'> 34 34 36\n",
      "['Note', 'Borrower', 'Guarantors', 'Facility', 'Project', 'Lender', 'Purpose', 'Final Maturity', 'Availability Period', 'Drawdown', 'Interest Margin', 'Interest', 'Front End Fee', 'Commitment Fee', 'Repayment', 'Voluntary Prepayment', 'Mandatory Prepayment', 'Voluntary Cancellation', 'Mandatory Cancellation', 'Security', 'Representations', 'Financial Covenants', 'Other Covenants and undertakings', 'Conditions Precedent', 'Status', 'Event of Default', 'Expenses', 'Documentation', 'FATCA', 'Transferability', 'Taxation', 'Governing Law', 'Jurisdiction', 'Solicitors']\n",
      "['Availability    Tranche A', 'undertakings', 'Period”         Tranche B', 'Cancellation', '[MB', 'Covenants', 'Prepayment', 'Precedent', 'Period', '- “Facilities”         [MB']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Errno 2] No such file or directory: 'data/annotated_ts/bfilled/21_BL_SYN_TS_mktd_antd_20220819_docparse.csv'\n",
      "<_io.BufferedReader name='pdf/annotated_ts/31_GL_PRJ_TS_mkd_antd_20220630.pdf'> 31 31 34\n",
      "['Borrower', 'Guarantor', 'Lender', 'Facility', 'Facility Amount', 'Purpose', 'Availability Period', 'Compliance Date', 'Drawdown', 'Initial Margin', 'Sustainabilit y advisory fee', 'All In Cost', 'Sustainabilit y', 'Commitment Fee', 'Interest Rate and Interest Period', 'Market disruption', 'Repayment', 'Cancellation', 'Prepayment', 'Financial Covenants', 'Other Covenants & Undertaking s', 'Guarantee', 'Status', 'Taxation', 'Increased costs', 'Documentati on', 'Expenses', 'Governing Law', 'Legal Counsel to Lender', 'Annex', 'Negative pledge']\n",
      "['(A) FATCA Deduction', 'Governing', 'Initial', 'Documentati', '(B) FATCA information', 'Sustainabilit', 'Market', 'Availability', '\"FATCA\" means', 'Other', 'Maturity         Tranche B', 'Legal', '5) Insert as a new clause', 'Amount           Tranche B', 'Compliance', 'Increased', 'Interest', 'Financial', 'Commitment', 'Final']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<_io.BufferedReader name='pdf/annotated_ts/46_AF_PRJ_TS_mkd_antd_20161013.pdf'> 50 50 57\n",
      "['Borrower/Lessor', 'Guarantor/Parent', 'Lessee', 'Aircraft', 'Aircraft Registration', 'Lease', 'Facility', 'Appraised Value', 'Currency', 'Purpose', 'Arranger', 'Facility Agent', 'Security Agent', 'Account Agent', 'Original Lender', 'Manufacturer', 'Availability Period', 'Drawdown', 'Interest Margin', 'Interest', 'Repayment', 'Final Maturity Date', 'Default Interest', 'Front-end Fee or Arrangement Fee', 'Facility Agency Fee &', 'Security Agency Fee', 'Interest Conversion', 'Cancellation', 'Prepayment', 'Mandatory Prepayment', 'Prepayment Fee', 'Conditions Precedent', 'Conditions Subsequent', 'Rental', 'Security', 'Insurances', 'Undertakings', 'Information Undertakings', 'Events of Default', 'Quiet Enjoyment', 'Operations, Maintenance and Inspection', 'Registration', 'Pooling and Sublease Rights', 'Taxes and Deductions', 'Transferability', 'Documentation', 'Expenses', 'Business Days', 'Legal Counsel', 'Governing Law']\n",
      "['Legend', 'Subsequent', 'Statistics', 'Sublease Rights', 'Inspection', 'SH', 'September 14, 2016 7', 'Document 2 ID  PowerDocs', 'Appraised', 'Deductions', 'Precedent', 'Input', 'Document 1 ID  PowerDocs', 'Arrangement Fee', 'SH comment', 'Mandatory Prepayment section. [B&M']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Errno 2] No such file or directory: 'data/annotated_ts/bfilled/15_AW_SYN_TS_mkd_20220826_docparse.csv'\n",
      "[Errno 2] No such file or directory: 'data/annotated_ts/bfilled/44_AF_SYN_TS_mkd_20151101_docparse.csv'\n",
      "[Errno 2] No such file or directory: 'data/annotated_ts/bfilled/63_AW_SYN_TS_atnd_mkd_20221025_docparse.csv'\n",
      "<_io.BufferedReader name='pdf/annotated_ts/17_NBFI_SYN_TS_mkd_antd_20221123.pdf'> 22 22 24\n",
      "['Borrower', 'Coordinator', 'Mandated Lead Arranger and Bookrunner', 'Lenders', 'Lenders. Agent', 'Purpose', 'Facility', 'Facility Amount', 'Signing Date', 'Maturity Date', 'Availability', 'Repayment', 'the Borrower shall immediately prepay all outstanding loans or that Lenders', 'Cancellation', 'Commitment Fee Waived. Interest Rate', 'Interest Period', 'Interest Calculation and Payment', 'Approvals', 'Withholding Taxes', 'Documentation', 'Covenants', 'Governing Law']\n",
      "['(“MLAB”)', 'Agent', 'Commitment Fee', 'and Payment', 'Prepayment', 'Financial Covenants', 'Interest Rate']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<_io.BufferedReader name='pdf/annotated_ts/55_VF_PRJ_TS_mkd_antd_20081113.pdf'> 26 26 27\n",
      "['Borrower', 'Lender', 'Guarantor', 'Vessel Owner/ Buyer of the Vessel', 'Charterer', 'Standby Charterer', 'Facility Amount', 'Purpose', 'Availability Period', 'Drawdown', 'Maturity Date', 'Repayment', 'Prepayment', 'Interest', 'Interest Period', 'Default Interest', 'Handling Charges', 'Commitment Fee', 'Conditions Precedent', 'Other Conditions', 'Financial Covenants', 'Payment without Deduction', 'Costs and Expenses', 'Documentation', 'Governing Law', 'Legal Counsel']\n",
      "['Payment       without', 'Vessel', 'Vessel        Owner/']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<_io.BufferedReader name='pdf/annotated_ts/41_AF_SYN_TS_mkd_antd_20190307.pdf'> 22 22 24\n",
      "['Aircraft', 'Delivery Date', 'Borrower and Lessor', 'Lender', 'Arranger', 'Loan Amount', 'Availability Period', 'Loan Structure', 'Interest Margin', 'Interest', 'Arrangement Fee', 'Facility Agent and Security Trustee Fee', 'Commitment Fee', 'Default Interest', 'Repayment', 'Security', 'Material Adverse Effect', 'Sole Lender', 'Subleasing', 'Documentation', 'Governing Law', 'Legal Costs']\n",
      "['   [LENDER A] Sole Lender', '    Aircraft', '   Material Adverse Effect', '\\uf09f    Subleasing', 'Facility Agent and Security Trustee', 'Currency', 'Term of the Loan']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Errno 2] No such file or directory: 'data/annotated_ts/bfilled/47_AF_PRJ_TS_mkd_20161013_docparse.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'data/annotated_ts/bfilled/40_AF_SYN_TS_mkd20190130_docparse.csv'\n",
      "[Errno 2] No such file or directory: 'data/annotated_ts/bfilled/76_NBFI_SYN_TS_mkd_20211231_docparse.csv'\n",
      "[Errno 2] No such file or directory: 'data/annotated_ts/bfilled/annotated_ts.json'\n",
      "<_io.BufferedReader name='pdf/annotated_ts/12_NBFI_SYN_TS_mkd_antd_20221107.pdf'> 56 56 57\n",
      "['Borrower', 'Guarantor', 'Obligors', 'Parent', 'Group', 'nan', 'Mandated Lead Arrangers and Bookrunners', 'Lenders', 'Majority Lenders', 'Facility Agent', 'Finance Parties', 'Facility Type and Facility Amount', 'Currency', 'Purpose', 'Final Maturity Date', '1 st Extension Option', '2 nd Extension Option', 'Availability Period', 'Drawdown', 'Repayment', 'Voluntary Prepayment', 'Mandatory Prepayment Events', 'Voluntary Cancellation', 'Interest Margin', 'Interest Period', 'Interest Rate', 'Default Interest', 'Base Rate', 'Arrangement Fee 1 st Extension Fee', '2 nd Extension Fee', 'Commitment Fee', 'Taxes and Deductions', 'Representatio ns', 'Financial covenants', 'Information covenants', 'Other covenants', 'Events of Default', 'Material Adverse Effect', 'Conditions Precedent', 'Documentatio n', 'Transferability', 'Status', 'Business Days', 'Governing Law and Jurisdiction', 'Legal Counsel', 'RFR', 'Interest on Reference Rate Loans', 'Reference Rate Loan', 'Determination of Reference Rate', 'Lookback Period', 'RFR Banking Day', 'Fallback central bank rate', 'Central Bank Rate Adjustment', 'Zero floor', 'Market disruption', 'Break Costs']\n",
      "['', 'Rate Loans', 'referred to above. [CC', 'Agreement. [CC', 'ns', 'Effect', 'Jurisdiction', 'Cancellation', 'Adjustment', '[CC', 'Parties', 'Arrangers and       [MLAB 1]. [CC', '(“MLABs”)', 'Events', 'Prepayment', 'Deductions', 'CC', 'Option', 'Precedent', 'Margin', 'and Facility    (the “Facility”). [CC', 'Period', '”)]', 'Fee', 'rate', 'covenants', 'Default', 'Amount', 'Interest', 'Interest       USD', 'n', 'Reference Rate', 'Date', 'Arrangement   [CC']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Errno 2] No such file or directory: 'data/annotated_ts/bfilled/16_AW_SYN_TS_mktd_antd_20221221_docparse.csv'\n",
      "[Errno 2] No such file or directory: 'data/annotated_ts/bfilled/56_VF_PRJ_TS_mkd_20081113_docparse.csv'\n"
     ]
    }
   ],
   "source": [
    "for pdffile in os.listdir('pdf/annotated_ts/'):\n",
    "    try:\n",
    "        ts_f = re.sub('_mkd_antd', '_mkd', re.sub('_antd_mkd', '_mkd_antd', pdffile))\n",
    "        ts_f = re.sub('.pdf', '_docparse.csv', ts_f)\n",
    "        match_f = re.sub('.csv', '_results.csv', ts_f)\n",
    "\n",
    "        df_ts = pd.read_csv(os.path.join('data/annotated_ts/bfilled/', ts_f), encoding='utf8')\n",
    "        df_ts = df_ts.replace({np.nan: None, 'nan': None, 'None': None})\n",
    "        df_ts = df_ts[~df_ts.section.isna()]\n",
    "        df_match = pd.read_csv(os.path.join(match_path, match_f), encoding='utf8')\n",
    "        sec_ts = []\n",
    "        for s in df_ts.section:\n",
    "            if s.strip() not in sec_ts:\n",
    "                sec_ts.append(s.strip())\n",
    "        sec_match = [s.strip() for s in list(set(df_match.TS_term))]\n",
    "        n1 = len(sec_ts)\n",
    "        n2 = len(sec_match)\n",
    "\n",
    "        with open(os.path.join('pdf/annotated_ts/', pdffile), 'rb') as f:\n",
    "            pdf = pdftotext.PDF(f)\n",
    "        count = 0\n",
    "        regex = ':'\n",
    "        length = max([len(line) for line in re.split('\\n', page)])\n",
    "\n",
    "        pdf_results = []\n",
    "        for page in pdf:\n",
    "            for line in re.split('\\n', page):\n",
    "                if re.search(regex, line[:round(length/2)]):\n",
    "                    if len(line) - len(line.lstrip()) < 5:\n",
    "                        # count += 1\n",
    "                        pdf_results.append(\n",
    "                            re.split(regex,line[:round(length/2)])[0].strip()\n",
    "                        )\n",
    "                    else:\n",
    "                        pass\n",
    "                        # print(line)\n",
    "        pdf_results = list(set(pdf_results))\n",
    "        pdf_results = [re.sub('• |\\uf0b7 |\\(s\\)', '', item) for item in pdf_results]\n",
    "        count = len(pdf_results)\n",
    "        if count > n2:\n",
    "            print(f, n1, n2, count)\n",
    "            excluded2 = [e for e in pdf_results if e not in sec_ts]\n",
    "            print(sec_ts)\n",
    "            print(excluded2)\n",
    "            print('\\n\\n\\n')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# match_path = 'term_matching_csv/20230807/'\n",
    "# for f in os.listdir(match_path):\n",
    "#     if f.endswith('csv'):\n",
    "#         try:\n",
    "#             ts_file = os.path.join('data/annotated_ts/bfilled/', re.sub('_results.csv', '.csv',f))\n",
    "#             df_ts = pd.read_csv(ts_file, encoding='utf8')\n",
    "#             df_ts = df_ts.replace({np.nan: None, 'nan': None, 'None': None})\n",
    "#             df_ts = df_ts[~df_ts.section.isna()]\n",
    "#             df_match = pd.read_csv(os.path.join(match_path,f), encoding='utf8')\n",
    "#             sec_ts = []\n",
    "#             for s in df_ts.section:\n",
    "#                 if s not in sec_ts:\n",
    "#                     sec_ts.append(s.strip())\n",
    "#             sec_match = [s.strip() for s in list(set(df_match.TS_term))]\n",
    "#             n1 = len(sec_ts)\n",
    "#             n2 = len(sec_match)\n",
    "\n",
    "#     #         excluded = [item for item in sec_ts if item not in sec_match]\n",
    "#     #         print(excluded)\n",
    "\n",
    "#             pdf_filename = re.sub('_docparse_results.csv', '.pdf', f)\n",
    "#             pdf_filename = re.sub('_mkd', '_mkd_antd', pdf_filename)\n",
    "#             with open(os.path.join('pdf/annotated_ts/', pdf_filename), 'rb') as pdffile:\n",
    "#                 pdf = pdftotext.PDF(pdffile)\n",
    "#             count = 0\n",
    "#             regex = ':'\n",
    "#             length = max([len(line) for line in re.split('\\n', page)])\n",
    "            \n",
    "#             pdf_results = []\n",
    "#             for page in pdf:\n",
    "#                 for line in re.split('\\n', page):\n",
    "#                     if re.search(regex, line[:round(length/2)]):\n",
    "#                         if len(line) - len(line.lstrip()) < 5:\n",
    "#                             count += 1\n",
    "#                             pdf_results.append(\n",
    "#                                 re.split(regex,line[:round(length/2)])[0].strip()\n",
    "#                             )\n",
    "#                         else:\n",
    "#                             pass\n",
    "#                             # print(line)\n",
    "            \n",
    "#             if count > n2:\n",
    "#                 print(f, n1, n2, count)\n",
    "#                 excluded2 = [e for e in pdf_results if e not in sec_ts]\n",
    "#                 print(sec_ts)\n",
    "#                 print(excluded2)\n",
    "#                 print('\\n\\n\\n')\n",
    "# #                 print(sec_ts)\n",
    "# #                 print('-'*50)\n",
    "# #                 print(pdf_results)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1_GL_SYN_TS_mkd_20221215_docparse_results.csv'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pdf/annotated_ts/1_GL_SYN_TS_mkd_antd_20221215.pdf\", \"rb\") as f:\n",
    "    pdf = pdftotext.PDF(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "                           Tranche A: €[50] million term loan facility (the “Euro\n",
      "                           Tranche B: US$[248] million term loan facility (the “US$\n",
      "                      limited to:\n",
      "                            less:         but more than\n",
      "                  Euro Base Rate: EURIBOR, with floor at 0%. [Clause 1.1\n",
      "                     basis:\n",
      "                       (Information: miscellaneous)]; (v) notice of any change in\n",
      "                       (Information: miscellaneous)]; (vi) notice of any Event of\n",
      "                        Guarantee: PRC Law. [To be documented under the PRC\n",
      "            follows:\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "regex = ':'\n",
    "length = max([len(line) for line in re.split('\\n', page)])\n",
    "print(length)\n",
    "\n",
    "for page in pdf:\n",
    "    for line in re.split('\\n', page):\n",
    "        # length = len(line)\n",
    "        if re.search(regex, line[:round(length/2)]):\n",
    "            if len(line) - len(line.lstrip()) < 5:\n",
    "                count += 1\n",
    "                #print(line[:round(length/2)])\n",
    "            else:\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-3bd5fb28ace2>, line 431)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-3bd5fb28ace2>\"\u001b[0;36m, line \u001b[0;32m431\u001b[0m\n\u001b[0;31m    except Exception as e:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# TS term section vs. FA definition\n",
    "top_N = 5\n",
    "\n",
    "ts_section_list = list(set(df_ts.processed_section))\n",
    "ts_section_list = [s for s in ts_section_list if s]\n",
    "def_string_list = list(set(df_def.definition))\n",
    "def_string_list = [s for s in def_string_list if s]\n",
    "\n",
    "pairs_def = dict()\n",
    "if ts_section_list and def_string_list:\n",
    "    pairs_def = get_similarity(\n",
    "        ts_section_list,\n",
    "        def_string_list,\n",
    "        \"sec_to_def\",\n",
    "        sim_threshold=0.9\n",
    "    )\n",
    "# else:\n",
    "#     print(f, 'Check: No definition')\n",
    "\n",
    "# TS term section vs. FA parties\n",
    "parties_string_list = []\n",
    "for s in list(set(df_parties.definition)):\n",
    "    if s:\n",
    "        if s not in parties_string_list:\n",
    "            if isinstance(s, str):\n",
    "                parties_string_list.append(s)\n",
    "pairs_parties = dict()\n",
    "if ts_section_list and parties_string_list:\n",
    "    pairs_parties = get_similarity(\n",
    "        ts_section_list,\n",
    "        parties_string_list,\n",
    "        \"sec_to_parties\",\n",
    "        sim_threshold=0.5\n",
    "    )\n",
    "# else:\n",
    "#     print(f, 'Check: No parties')\n",
    "\n",
    "# TS term v.s. FA sub section\n",
    "sub_sec_list = list(set(df_clause[df_clause.text_element == \"sub_section\"].sub_section))\n",
    "sub_sec_list = [s for s in sub_sec_list if s]\n",
    "pairs_sec_to_sub_sec = dict()\n",
    "if ts_section_list and sub_sec_list:\n",
    "    pairs_sec_to_sub_sec = get_similarity(\n",
    "        ts_section_list,\n",
    "        sub_sec_list,\n",
    "        \"sec_to_sub_sec\",\n",
    "        sim_threshold=0\n",
    "    )\n",
    "# else:\n",
    "#     print(f, 'Check: No sub-section')\n",
    "# TS term + text vs. FA clause\n",
    "clause_section_list = list(set(df_clause[df_clause.text_element == \"section\"].section))\n",
    "clause_section_list = [s for s in clause_section_list if s]\n",
    "# TS section v.s. FA clause section -> select potential FA clause section\n",
    "\n",
    "total_pairs_clause = []\n",
    "pairs_clause_section = dict()\n",
    "if ts_section_list and clause_section_list:\n",
    "    pairs_clause_section = get_similarity(\n",
    "        ts_section_list,\n",
    "        clause_section_list,\n",
    "        \"clause_section\",\n",
    "        sim_threshold=0.3\n",
    "    )\n",
    "# else:\n",
    "#     print(f, 'Check: No section')\n",
    "    # check under the section candidates\n",
    "    for k, v in pairs_clause_section.items():\n",
    "        df_ts_sub = df_ts[df_ts.processed_section == k]    \n",
    "        # TODO: improve this part, duplicates         \n",
    "        ts_text_list = [] # process nan value\n",
    "        for s in list(set(df_ts_sub[df_ts_sub.text_element!='section'].text)):\n",
    "            if s:\n",
    "                if s not in ts_text_list:\n",
    "                    if isinstance(s, str):\n",
    "                        ts_text_list.append(s)\n",
    "        sub_ts_section_list = list(set(df_ts_sub.processed_section))\n",
    "\n",
    "        candidates = [item['similar_term'] for item in v]\n",
    "        df_clause_sub = df_clause[df_clause.section.isin(candidates)]\n",
    "        # print(f'ts_section: {k}, ts_section_length: {len(df_ts_sub)}, ts_text_length: {len(ts_text_list)}, fa_section_length: {len(df_clause_sub)}')\n",
    "\n",
    "        # TODO: improve this part\n",
    "        sub_section_list = [] # process nan value\n",
    "        for s in list(set(df_clause_sub[df_clause_sub.text_element == \"sub_section\"].sub_section)):\n",
    "            if s:\n",
    "                if s not in sub_section_list:\n",
    "                    if isinstance(s, str):\n",
    "                        sub_section_list.append(s)\n",
    "        clause_string_list = list(set(\n",
    "            df_clause_sub[(df_clause_sub.text_element != \"section\") & (df_clause_sub.text_element != \"sub_section\")].text\n",
    "        ))\n",
    "\n",
    "        pairs_sub_section = dict()\n",
    "        # pairs_sec_to_sub_sec_partial = dict()\n",
    "        if sub_section_list:\n",
    "            if ts_text_list:\n",
    "                pairs_sub_section = get_similarity(\n",
    "                    ts_text_list,\n",
    "                    sub_section_list,\n",
    "                    \"text_to_sub_sec\",\n",
    "                    sim_threshold=0\n",
    "                )\n",
    "            # else:\n",
    "            #     print('no text in ', k)\n",
    "            # if sub_ts_section_list:\n",
    "            #     pairs_sec_to_sub_sec_partial = get_similarity(\n",
    "            #         sub_ts_section_list,\n",
    "            #         sub_section_list,\n",
    "            #         \"sec_to_sub_sec\",\n",
    "            #         sim_threshold=0\n",
    "            #     )\n",
    "            # else:\n",
    "            #     print('no section in ', k)\n",
    "\n",
    "        # else:\n",
    "        #     print('no sub section in ', k)\n",
    "        # if sub_sec_list:\n",
    "        #     pairs_sec_to_sub_sec.update(pairs_sec_to_sub_sec_partial)\n",
    "        pairs_clause = dict()\n",
    "        if ts_text_list and clause_string_list:\n",
    "            pairs_clause = get_similarity(\n",
    "                ts_text_list,\n",
    "                clause_string_list,\n",
    "                \"text_to_clause_text\",\n",
    "                sim_threshold=0\n",
    "            )\n",
    "\n",
    "        if pairs_sub_section:\n",
    "            # case: same text under different terms\n",
    "            total_pairs_clause.append({\n",
    "                k: pairs_sub_section\n",
    "            })\n",
    "        if pairs_clause:\n",
    "            total_pairs_clause.append({k: pairs_clause})\n",
    "\n",
    "### add TS term + text vs. FA schedule\n",
    "# TODO: use the whole schedule or the details?\n",
    "# 0729: add \"part\" in schedule; TS term vs. FA schedule part\n",
    "ts_section_list = list(set(df_ts.processed_section))\n",
    "ts_section_list = [s for s in ts_section_list if s]\n",
    "schedule_section_list = list(set(df_schedule.schedule))\n",
    "schedule_part_list = []\n",
    "\n",
    "for s in list(set(df_schedule.part)):\n",
    "    if s:\n",
    "        if s not in schedule_part_list:\n",
    "            if isinstance(s, str):\n",
    "                schedule_part_list.append(s)\n",
    "\n",
    "pairs_schedule_section = dict()\n",
    "if ts_section_list and schedule_section_list:\n",
    "    pairs_schedule_section = get_similarity(\n",
    "        ts_section_list,\n",
    "        schedule_section_list,\n",
    "        \"schedule_section\",\n",
    "        sim_threshold=0.5\n",
    "    )\n",
    "# else:\n",
    "#     print('No schedule section')\n",
    "\n",
    "pairs_schedule_part = dict()\n",
    "if ts_section_list and schedule_part_list:\n",
    "    pairs_schedule_part = get_similarity(\n",
    "        ts_section_list,\n",
    "        schedule_part_list,\n",
    "        \"schedule_part\",\n",
    "        sim_threshold=0\n",
    "    )\n",
    "# else:\n",
    "#     print('No schedule part')\n",
    "\n",
    "total_pairs_sched = []\n",
    "if ts_section_list and schedule_section_list:\n",
    "    # check under the section candidates\n",
    "    for k, v in pairs_schedule_section.items():\n",
    "        df_ts_sub = df_ts[df_ts.processed_section == k]\n",
    "        ts_text_list = list(set(df_ts_sub[df_ts_sub.text_element!='section'].text))\n",
    "\n",
    "        candidates = [item['similar_term'] for item in v]\n",
    "        df_sched_sub = df_schedule[df_schedule.schedule.isin(candidates)]\n",
    "        sched_text_list = list(set(df_sched_sub.text))\n",
    "        pairs_sched = dict()\n",
    "        # print(f'ts_text_length: {len(ts_text_list)}, schedule_text_length: {len(sched_text_list)}')\n",
    "        if ts_text_list and sched_text_list:\n",
    "            pairs_sched = get_similarity(\n",
    "                ts_text_list,\n",
    "                sched_text_list,\n",
    "                \"text_to_schedule_text\",\n",
    "                sim_threshold=0\n",
    "            )\n",
    "        if pairs_sched:\n",
    "            total_pairs_sched.append({k: pairs_sched})\n",
    "\n",
    "# summarize all results\n",
    "\n",
    "df_ts['similar_def'] = df_ts['processed_section'].apply(\n",
    "    lambda i: pairs_def.get(i)\n",
    ")\n",
    "df_ts['similar_parties'] = df_ts['processed_section'].apply(\n",
    "    lambda i: pairs_parties.get(i)\n",
    ")\n",
    "df_ts['similar_sub_section'] = df_ts['processed_section'].apply(\n",
    "    lambda i: pairs_sec_to_sub_sec.get(i) if sub_sec_list else None\n",
    ")\n",
    "df_ts['similar_schedule'] = df_ts['processed_section'].apply(\n",
    "    lambda i: pairs_schedule_part.get(i)\n",
    ")\n",
    "df_ts['similar_sched_section'] = df_ts['processed_section'].apply(\n",
    "    lambda i: pairs_schedule_section.get(i)\n",
    ")\n",
    "\n",
    "df_ts_def = df_ts[~df_ts.similar_def.isna()][['section', 'processed_section', 'text','similar_def']]\n",
    "df_ts_parties = df_ts[~df_ts.similar_parties.isna()][['section', 'processed_section',  'text','similar_parties']]\n",
    "df_ts_sub_sec = df_ts[~df_ts.similar_sub_section.isna()][['section', 'processed_section',  'text', 'similar_sub_section']]\n",
    "df_ts_sched = df_ts[~df_ts.similar_schedule.isna()][['section', 'processed_section',  'text', 'similar_schedule']]\n",
    "df_ts_sched_sec = df_ts[~df_ts.similar_sched_section.isna()][['section', 'processed_section',  'text', 'similar_sched_section']]\n",
    "\n",
    "### check\n",
    "total_pairs = []\n",
    "\n",
    "for sec in list(set(df_ts_def.processed_section)):\n",
    "    sub_df = df_ts_def[df_ts_def.processed_section==sec]\n",
    "    total_pairs.append({\n",
    "        sec: dict(zip(sub_df.text, sub_df.similar_def))\n",
    "    })\n",
    "\n",
    "for sec in list(set(df_ts_parties.processed_section)):\n",
    "    sub_df = df_ts_parties[df_ts_parties.processed_section==sec]\n",
    "    total_pairs.append({\n",
    "        sec: dict(zip(sub_df.text, sub_df.similar_parties))\n",
    "    })\n",
    "\n",
    "for sec in list(set(df_ts_sub_sec.processed_section)):\n",
    "    sub_df = df_ts_sub_sec[df_ts_sub_sec.processed_section==sec]\n",
    "    total_pairs.append({\n",
    "        sec: dict(zip(sub_df.text, sub_df.similar_sub_section))\n",
    "    })\n",
    "for sec in list(set(df_ts_sched.processed_section)):\n",
    "    sub_df = df_ts_sched[df_ts_sched.processed_section==sec]\n",
    "    total_pairs.append({\n",
    "        sec: dict(zip(sub_df.text, sub_df.similar_schedule))\n",
    "    })\n",
    "for sec in list(set(df_ts_sched_sec.processed_section)):\n",
    "    sub_df = df_ts_sched_sec[df_ts_sched_sec.processed_section==sec]\n",
    "    total_pairs.append({\n",
    "        sec: dict(zip(sub_df.text, sub_df.similar_sched_section))\n",
    "    })\n",
    "\n",
    "total_pairs.extend(total_pairs_clause)\n",
    "total_pairs.extend(total_pairs_sched)\n",
    "\n",
    "\n",
    "keys = []\n",
    "for pair in total_pairs:\n",
    "    k = list(pair.keys())[0]\n",
    "    if k not in keys:\n",
    "        keys.append(k)\n",
    "\n",
    "\n",
    "total_pairs_updated = []\n",
    "for k in keys:\n",
    "    sub_pairs = [p[k] for p in total_pairs if list(p.keys())[0] == k]\n",
    "    dd = defaultdict(list)\n",
    "    for p in sub_pairs:\n",
    "        for i, j in p.items():\n",
    "            dd[i].extend(j)\n",
    "\n",
    "    total_pairs_updated.append({k: dd})\n",
    "\n",
    "results = []\n",
    "\n",
    "for pair in total_pairs_updated:\n",
    "    for sec, value in pair.items():\n",
    "        for text, match in value.items():\n",
    "            for item in match:\n",
    "                results.append({\n",
    "                    'TS_section': sec,\n",
    "                    'TS_text': text,\n",
    "                    'match_term': item['similar_term'],\n",
    "                    'similarity': item['score'],\n",
    "                    'match_type': item['map_type']\n",
    "                })\n",
    "\n",
    "df_results = pd.DataFrame(data=results)\n",
    "# print(json.dumps(results,indent=4))\n",
    "# df_results = df_results.sort_values(by=['TS_text', 'similarity'], ascending=False)\n",
    "\n",
    "ts_map = {}\n",
    "for idx, row in df_ts[['index','text_block_id','page_id', 'phrase_id', 'section','processed_section','text']].drop_duplicates().iterrows():\n",
    "    ts_map[row['text']] = [\n",
    "        row['index'],\n",
    "        row['text_block_id'],\n",
    "        row['page_id'],\n",
    "        row['phrase_id']\n",
    "    ]\n",
    "\n",
    "content2id = {\n",
    "    'sec_to_def': dict(),\n",
    "    'text_to_clause_text': dict(),\n",
    "    'text_to_sub_sec': dict(),\n",
    "    'text_to_schedule_text': dict(),\n",
    "    'sec_to_parties': dict(),\n",
    "    'sec_to_sub_sec': dict(),\n",
    "    'schedule_part': dict(),\n",
    "    'schedule_section': dict()\n",
    "}\n",
    "\n",
    "for idx, row in df_def[['definition', 'identifier']].drop_duplicates().iterrows():\n",
    "    content2id['sec_to_def'].update({row['definition']: row['identifier']})\n",
    "\n",
    "for idx, row in df_fa[df_fa.text_element=='sub_section'][['sub_section', 'identifier']].drop_duplicates().iterrows():\n",
    "    content2id['text_to_sub_sec'].update({row['sub_section']: row['identifier']})\n",
    "content2id['sec_to_sub_sec'] = content2id['text_to_sub_sec']\n",
    "for idx, row in df_clause[['text', 'identifier']].drop_duplicates().iterrows():\n",
    "    content2id['text_to_clause_text'].update({row['text']: row['identifier']})\n",
    "\n",
    "\n",
    "for idx, row in df_schedule[['text', 'identifier']].drop_duplicates().iterrows():\n",
    "    content2id['text_to_schedule_text'].update({row['text']: row['identifier']})\n",
    "for idx, row in df_schedule[['part', 'identifier']].drop_duplicates().iterrows():\n",
    "    content2id['schedule_part'].update({row['part']: row['identifier']})\n",
    "for idx, row in df_schedule[df_schedule.text_element=='section'][['schedule', 'identifier']].drop_duplicates().iterrows():\n",
    "    content2id['schedule_section'].update({row['schedule']: row['identifier']})\n",
    "\n",
    "for idx, row in df_parties[['definition', 'identifier']].drop_duplicates().iterrows():\n",
    "    content2id['sec_to_parties'].update({row['definition']: row['identifier']})\n",
    "\n",
    "df_results['identifier'] = df_results.apply(\n",
    "    lambda i: content2id[i['match_type']].get(i['match_term']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_results['index'] = df_results.apply(\n",
    "    lambda i: ts_map.get(i['TS_text'])[0],\n",
    "    axis=1\n",
    ")\n",
    "df_results['text_block_id'] = df_results.apply(\n",
    "    lambda i: ts_map.get(i['TS_text'])[1],\n",
    "    axis=1\n",
    ")\n",
    "df_results['page_id'] = df_results.apply(\n",
    "    lambda i: ts_map.get(i['TS_text'])[2],\n",
    "    axis=1\n",
    ")\n",
    "df_results['phrase_id'] = df_results.apply(\n",
    "    lambda i: ts_map.get(i['TS_text'])[3],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_results = df_results.sort_values(\n",
    "    by=['TS_section', 'TS_text', 'similarity'],\n",
    "    ascending=False\n",
    ")\n",
    "df_results = df_results[~df_results.match_term.isna()]\n",
    "df_results = df_results.drop_duplicates()\n",
    "check_csv_path = os.path.join(OUTPUT_folderpath, check_date, 'check', f)\n",
    "df_results.to_csv(check_csv_path, index=False)\n",
    "\n",
    "final = []\n",
    "\n",
    "for term in list(set(df_results.TS_section)):\n",
    "    df_s = df_results[df_results.TS_section==term]\n",
    "    for text in list(set(df_s.TS_text)):\n",
    "        df_ss = df_s[df_s.TS_text==text]\n",
    "        try:\n",
    "            final.append({\n",
    "                'index': list(df_ss['index'])[0],\n",
    "                'text_block_id': list(df_ss['text_block_id'])[0],\n",
    "                'page_id': list(df_ss['page_id'])[0],\n",
    "                'phrase_id': list(df_ss['phrase_id'])[0],\n",
    "                'TS_term': term,\n",
    "                'TS_text': text,\n",
    "                'match_term_list': list(df_ss['match_term'])[:5],\n",
    "                'identifier_list': list(df_ss['identifier'])[:5],\n",
    "                'similarity_list': list(df_ss['similarity'])[:5],\n",
    "                'match_type_list': list(df_ss['match_type'])[:5]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f'TS_term: {term}, TS_text: {text}, Error: {e}')\n",
    "df_final = pd.DataFrame(data=final)\n",
    "save_f = re.sub('.csv', '_results.csv', f)\n",
    "results_csv_path = os.path.join(OUTPUT_folderpath, check_date, save_f)\n",
    "df_final.to_csv(results_csv_path, index=False)\n",
    "# print('results are saved to: ', save_f)\n",
    "end_time = time.perf_counter()\n",
    "total_time = end_time - start_time\n",
    "print(f'{f} matching time: {total_time} s')\n",
    "done_list.append(f)\n",
    "\n",
    "with open(os.path.join(OUTPUT_folderpath, f\"done_term_matching_list_{check_date}.json\"), \"w\") as outfile:\n",
    "    outfile.write(json.dumps(done_list,indent=4))\n",
    "except Exception as e:\n",
    "print(f, e)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
